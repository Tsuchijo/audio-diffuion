{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import os \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import spectrogram\n",
    "#import torchaudio \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3877/1907631780.py:9: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sample_rate, audio_data = wavfile.read(wav_file)\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import wavfile\n",
    "data_path = 'data/'\n",
    "## Only Diffusiing from a single sample (TODO: Load many samples)\n",
    "sample_name = os.listdir(data_path)[2]\n",
    "# Specify the path to the .wav file\n",
    "wav_file = os.path.join(data_path, sample_name)\n",
    "\n",
    "# Load the .wav file as a numpy array\n",
    "sample_rate, audio_data = wavfile.read(wav_file)\n",
    "# Avg together Left and Right channels to get Mono Signal\n",
    "audio_data = np.mean(audio_data, axis=1)\n",
    "## Transform to torch tensor\n",
    "audio_data = torch.from_numpy(audio_data).float().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Audio for playback\n",
    "from IPython.display import Audio\n",
    "## TODO: Figure out why certain combinations of n_mels cause the conversion to fail\n",
    "spectrogram_transforms = spectrogram.Spectrogram(\n",
    "    n_fft=1024,\n",
    "    win_length=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=80,\n",
    "    sample_rate=sample_rate,\n",
    ")\n",
    "\n",
    "## TODO: Add in VAE from audioldm\n",
    "\n",
    "mel_scale = spectrogram_transforms.mel_transform(audio_data)\n",
    "#reconstructed_audio = spectrogram_transforms.inverse_transform(mel_scale)\n",
    "\n",
    "## Split the mel_scale data from \n",
    "\n",
    "#Audio(torch.cat((audio_data, reconstructed_audio), dim=1), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape from 1x80xn to (n//1470)x80x1470\n",
    "# Calculate the desired size after reshaping\n",
    "N = mel_scale.size(2)\n",
    "new_size = (N // 1480, 1480, 80)\n",
    "remaining = N % 1480\n",
    "## cut the remaining part\n",
    "cut_mel_scale = mel_scale[:, :, :-remaining]\n",
    "# Reshape the padded tensor\n",
    "reshaped_mel_scale = cut_mel_scale.swapdims(1,2).reshape(new_size).swapdims(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create x axis based off sample rate\n",
    "time = np.arange(0, len(audio_data), 1) / sample_rate\n",
    "plt.plot(time, audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0248,  0.0207,  0.0211,  ...,  0.0391, -0.0187,  0.0258],\n",
       "          [ 0.0249,  0.0490,  0.0712,  ...,  0.0537,  0.0663,  0.0036],\n",
       "          [ 0.0567,  0.0305,  0.0260,  ...,  0.0430,  0.0373,  0.0505],\n",
       "          ...,\n",
       "          [ 0.0645,  0.0197,  0.0099,  ...,  0.0265,  0.1017,  0.0488],\n",
       "          [ 0.0041,  0.0016,  0.0313,  ...,  0.0328,  0.0424, -0.0242],\n",
       "          [ 0.0232,  0.0281,  0.0456,  ..., -0.0173,  0.0023,  0.0475]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Import DDPM and model for processing data\n",
    "# from Models.models import Mel_Convolv\n",
    "# from DDPM import DDPM_Scheduler\n",
    "# import torch\n",
    "\n",
    "# model = Mel_Convolv(80, 1470)\n",
    "# test = torch.rand(1, 1, 80, 1470)\n",
    "# model(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
